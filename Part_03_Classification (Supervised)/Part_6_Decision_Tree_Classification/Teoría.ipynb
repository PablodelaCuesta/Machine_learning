{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "Un árbol de decisión es una estructura de datos que se puede utilizar para establecer un conjunto de reglas de decisión ya que se puede representar de forma visual el conjunto de reglas a seguir.\n",
    "\n",
    "La estructura básica es un conjunto de nodos y un conjunto de ramas u hojas.\n",
    "\n",
    "Se usa cuando la variable objetivo es discreta o categórica. Mientras que las predictoras pueden ser categóricas como numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de las ventajas de los árboles de decisión son:\n",
    "\n",
    "* Fácil de *entender* e *interpretar*. Los árboles pueden ser **visualizados**.\n",
    "* *Requiere poca preparación de datos*. Otras técnicas requieren a menudo la normalización de los datos, la creación de variables ficticias y la eliminación de valores en blanco. Tenga en cuenta, sin embargo, que este módulo no admite los **valores perdidos**.\n",
    "* El costo de usar el árbol (es decir, predecir datos) es *logarítmico* en el número de puntos de datos usados para entrenar al árbol.\n",
    "* Capaz de manejar *datos numéricos y categóricos*. Otras técnicas suelen estar especializadas en el análisis de conjuntos de datos que sólo tienen un tipo de variable.\n",
    "* Capaz de manejar problemas de múltiples salidas.\n",
    "* Utiliza un modelo de *caja blanca*. Si una situación dada es observable en un modelo, la explicación de la condición se explica fácilmente por la lógica booleana. Por el contrario, en un modelo de caja negra (por ejemplo, en una red neural artificial), los resultados pueden ser más difíciles de interpretar.\n",
    "* Posibilidad de validar un modelo mediante **pruebas estadísticas**. Esto permite dar cuenta de la fiabilidad del modelo.\n",
    "* Funciona bien incluso si sus suposiciones son algo violadas por el verdadero modelo a partir del cual se generaron los datos.\n",
    "\n",
    "Las desventajas de los árboles de decisión incluyen:\n",
    "\n",
    "* Los árboles de decisiones pueden crear árboles demasiado complejos que no generalizan bien los datos. A esto se le llama **overfitting**. Mecanismos tales como la poda, la fijación del número mínimo de muestras requeridas en un nudo de la hoja o la fijación de la profundidad máxima del árbol son necesarios para evitar este problema.\n",
    "* Los árboles de decisión pueden ser **inestables** porque pequeñas variaciones en los datos pueden resultar en la generación de un árbol completamente diferente. Este problema se mitiga utilizando árboles de decisión dentro de un conjunto.\n",
    "* El problema de entrenar un árbol de decisión óptimo es conocido por ser NP-completo bajo varios aspectos de optimización e incluso para conceptos simples. En consecuencia, los algoritmos prácticos de aprendizaje del árbol de decisión se basan en algoritmos heurísticos como el algoritmo codicioso, en el que se toman decisiones óptimas a nivel local en cada nodo. Tales algoritmos no pueden garantizar que devuelvan el árbol de decisión globalmente óptimo. Esto puede ser mitigado entrenando a múltiples árboles en un grupo, donde las características y muestras son muestreadas al azar con reemplazo.\n",
    "* Hay conceptos que son difíciles de aprender porque los árboles de decisión no los expresan fácilmente, como XOR, paridad o problemas de multiplexor.\n",
    "* Los árboles de decisión crean árboles sesgados si algunas clases dominan. Por lo tanto, se recomienda equilibrar el conjunto de datos antes de ajustarlo al árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganancia de la información y entropía\n",
    "\n",
    "La entropía satisface las siguientes afirmaciones:\n",
    "\n",
    "* La medida de la información debe ser proporcional. Es decir, un cambio pequeño en una de las probabilidades de aparición de uno de los elementos debe cambiar poco la entropía.\n",
    "* Si todos los elementos de la señal son equiprobables, entonces la entropía será máxima.\n",
    "\n",
    "Definición formal:\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum_{i = 1}^n{p_i log_{2}(p_i)}\n",
    "$$\n",
    "\n",
    "Por lo tanto, la entropía de un mensaje $X$, denotado por $H(X)$, es el valor medio ponderado de la cantidad de información de los diversos estados del mensaje que representa una medida de la incertidumbre media acerca de una variable aleatoria y por tanto de la cantidad de información.\n",
    "\n",
    "### Coeficiente de Gini\n",
    "\n",
    "\n",
    "Medida de la desilgualdad, utilizada inicialmente para medir la desigualdad en los ingresos, pero que puede utilizarse para medir cualquier forma de distribución desigual.\n",
    "\n",
    "* El valor $0$ corresponde con la perfecta igualdad\n",
    "* El valor $1$ corresponde con la perfecta desigualdad\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier\n",
    "\n",
    "Al igual que con otros clasificadores, **DecisionTreeClassifier** toma como entrada dos matrices: una matriz X, dispersa o densa, de tamaño `[n_muestras, n_características]` que contiene las muestras de formación, y una matriz Y de valores enteros, tamaño `[n_muestras]`, que contiene las etiquetas de clase para las muestras de formación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# biblioteca para visualización de grafos\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Llamamos a la clase\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# entrenamos el árbol\n",
    "clf = clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenados, podemos exportar el árbol en formato Graphviz usando el exportador export_graphviz. Si utiliza el gestor de paquetes conda, los binarios graphviz y el paquete python pueden instalarse con:\n",
    "\n",
    "    conda install python-graphviz\n",
    "\n",
    "Abajo podemos ver un ejemplo de exportación de un árbol entrenado; los resultados se guardan en un archivo llamado *iris.dot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "#graph No funciona en Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/Iris.gv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el criterio de la entropía:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=99,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_entropy = tree.DecisionTreeClassifier(criterion='entropy',\n",
    "                                           min_samples_split=20,\n",
    "                                          random_state=99)\n",
    "tree_entropy.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree.export_graphviz(tree_entropy, out_file='resources/iris_entropy.gv', \n",
    "                    feature_names=iris.feature_names,  \n",
    "                    class_names=iris.target_names,  \n",
    "                    filled=True, rounded=True,  \n",
    "                    special_characters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"graphviz.render(engine='dot',\\n                filepath='resources/iris_entropy.gv',\\n                format='png')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"graphviz.render(engine='dot',\n",
    "                filepath='resources/iris_entropy.gv',\n",
    "                format='png')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](resources/iris_entropy.gv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=20,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=99,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytree = tree.DecisionTreeClassifier(criterion='entropy', \n",
    "                                    max_depth = 5,\n",
    "                                    min_samples_split = 20,\n",
    "                                    random_state = 99)\n",
    "mytree.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usamos el método K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n = X.shape[0], n_folds=10, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.mean(cross_val_score(mytree, X, y, scoring=\"accuracy\", cv = cv, n_jobs = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar un bucle for para validar sobre distintas niveles de profundidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para profundidad = 1, es de 0.5666666666666667\n",
      "     [0. 0. 1. 0.]\n",
      "\n",
      "Score para profundidad = 2, es de 0.9200000000000002\n",
      "     [0.         0.         0.66620285 0.33379715]\n",
      "\n",
      "Score para profundidad = 3, es de 0.9400000000000001\n",
      "     [0.         0.         0.68976981 0.31023019]\n",
      "\n",
      "Score para profundidad = 4, es de 0.9333333333333333\n",
      "     [0.         0.         0.66869158 0.33130842]\n",
      "\n",
      "Score para profundidad = 5, es de 0.9333333333333333\n",
      "     [0.         0.         0.66869158 0.33130842]\n",
      "\n",
      "Score para profundidad = 6, es de 0.9333333333333333\n",
      "     [0.         0.         0.66869158 0.33130842]\n",
      "\n",
      "Score para profundidad = 7, es de 0.9333333333333333\n",
      "     [0.         0.         0.66869158 0.33130842]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1,8):\n",
    "    mytree = tree.DecisionTreeClassifier(criterion='entropy', \n",
    "                                    max_depth = depth,\n",
    "                                    min_samples_split = 20,\n",
    "                                    random_state = 99)\n",
    "    mytree.fit(X,y)\n",
    "    cv = KFold(n = X.shape[0], n_folds=10, shuffle=True, random_state=1)\n",
    "    score = np.mean(cross_val_score(mytree, X, y, scoring=\"accuracy\", cv = cv, n_jobs = 1))\n",
    "    print(\"Score para profundidad = {0}, es de {1}\".format(depth, np.mean(score)))\n",
    "    print(\"    \", mytree.feature_importances_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que si únicamente pudiéramos elegir una de las variables predictoras, la más conveniente sería la longitud de los pétalos. En cambio si podemos elegir entre dos o más variables predictoras, las más convenientes son la longitud de los pétalos y su anchura."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
